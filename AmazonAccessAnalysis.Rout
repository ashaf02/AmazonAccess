
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # Hint: 112
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ recipes      1.0.8
✔ dials        1.2.0     ✔ rsample      1.2.0
✔ dplyr        1.1.3     ✔ tibble       3.2.1
✔ ggplot2      3.4.3     ✔ tidyr        1.3.0
✔ infer        1.0.5     ✔ tune         1.1.2
✔ modeldata    1.2.0     ✔ workflows    1.1.3
✔ parsnip      1.1.1     ✔ workflowsets 1.0.1
✔ purrr        1.0.2     ✔ yardstick    1.2.0
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ purrr::discard() masks scales::discard()
✖ dplyr::filter()  masks stats::filter()
✖ dplyr::lag()     masks stats::lag()
✖ recipes::step()  masks stats::step()
• Use tidymodels_prefer() to resolve common conflicts.
> library(embed)
> library(vroom)

Attaching package: ‘vroom’

The following object is masked from ‘package:yardstick’:

    spec

The following object is masked from ‘package:scales’:

    col_factor

> 
> ## Read in the data
> amazonTrain <- vroom("amazon.train.csv")
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> amazonTest <- vroom("amazon.test.csv")
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> amazon_recipe <- recipe(ACTION~., data=amazonTrain) %>%
+ step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
+   step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
+   step_dummy(all_nominal_predictors()) %>%# dummy variable encoding
+   step_mutate(ACTION=as.factor(ACTION), skip=TRUE)
>   
> 
> # NOTE: some of these step functions are not appropriate to use together
> 
> # apply the recipe to your data
> prep <- prep(amazon_recipe)
> baked <- bake(prep, new_data = amazonTrain)
> 
> ## Logistic Regression
> 
> my_mod <- logistic_reg() %>% #Type of model
+   set_engine("glm")
> 
> amazon_workflow <- workflow() %>%
+ add_recipe(amazon_recipe) %>%
+ add_model(my_mod) %>%
+ fit(data = amazonTrain) # Fit the workflow
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
> 
> amazon_predictions <- predict(amazon_workflow,
+                               new_data=amazonTest,
+                               type="class") # "class" or "prob" (see doc)
Warning message:
In predict.lm(object, newdata, se.fit, scale = 1, type = if (type ==  :
  prediction from rank-deficient fit; attr(*, "non-estim") has doubtful cases
> 
> amazon_predictions <- cbind(amazonTest$id, amazon_predictions) %>%
+   rename(Id = "amazonTest$id",
+   Action = ".pred_class") #%>%
>   #select (-"amazonTest$id")
> 
> head(amazon_predictions)
  Id Action
1  1      1
2  2      1
3  3      1
4  4      1
5  5      1
6  6      1
> 
> vroom_write(x=amazon_predictions, file="logisticpreds3.csv", delim=",")
> # save(file="filename.RData", list=c("logReg_wf"))
> # load("filename.RData")
> 
> ## Penalized Logistic Regression
> 
> # Define the model
> my_mod <- logistic_reg(mixture = tune(), penalty = tune()) %>%
+   set_engine("glmnet")
> 
> # Create the workflow
> amazon_workflow <- workflow() %>%
+   add_recipe(amazon_recipe) %>%
+   add_model(my_mod)
> 
> # Define the hyperparameter grid
> tuning_grid <- grid_regular(penalty(), mixture(), levels = 3) # Adjust the number of levels as needed
> 
> # Split data for cross-validation
> folds <- vfold_cv(amazonTrain, v = 5, repeats = 1)
> 
> # Run cross-validation
> CV_results <- amazon_workflow %>%
+   tune_grid(
+     resamples = folds,
+     grid = tuning_grid,
+     metrics = metric_set(roc_auc),
+     control = control_grid(verbose = TRUE)  # Enable verbose output to monitor progress
+   )
i Fold1: preprocessor 1/1
✓ Fold1: preprocessor 1/1
i Fold1: preprocessor 1/1, model 1/3
✓ Fold1: preprocessor 1/1, model 1/3
i Fold1: preprocessor 1/1, model 1/3 (extracts)
i Fold1: preprocessor 1/1, model 1/3 (predictions)
i Fold1: preprocessor 1/1, model 2/3
✓ Fold1: preprocessor 1/1, model 2/3
i Fold1: preprocessor 1/1, model 2/3 (extracts)
i Fold1: preprocessor 1/1, model 2/3 (predictions)
i Fold1: preprocessor 1/1, model 3/3
✓ Fold1: preprocessor 1/1, model 3/3
i Fold1: preprocessor 1/1, model 3/3 (extracts)
i Fold1: preprocessor 1/1, model 3/3 (predictions)
i Fold2: preprocessor 1/1
✓ Fold2: preprocessor 1/1
i Fold2: preprocessor 1/1, model 1/3
✓ Fold2: preprocessor 1/1, model 1/3
i Fold2: preprocessor 1/1, model 1/3 (extracts)
i Fold2: preprocessor 1/1, model 1/3 (predictions)
i Fold2: preprocessor 1/1, model 2/3
✓ Fold2: preprocessor 1/1, model 2/3
i Fold2: preprocessor 1/1, model 2/3 (extracts)
i Fold2: preprocessor 1/1, model 2/3 (predictions)
i Fold2: preprocessor 1/1, model 3/3
✓ Fold2: preprocessor 1/1, model 3/3
i Fold2: preprocessor 1/1, model 3/3 (extracts)
i Fold2: preprocessor 1/1, model 3/3 (predictions)
i Fold3: preprocessor 1/1
✓ Fold3: preprocessor 1/1
i Fold3: preprocessor 1/1, model 1/3
✓ Fold3: preprocessor 1/1, model 1/3
i Fold3: preprocessor 1/1, model 1/3 (extracts)
i Fold3: preprocessor 1/1, model 1/3 (predictions)
i Fold3: preprocessor 1/1, model 2/3
✓ Fold3: preprocessor 1/1, model 2/3
i Fold3: preprocessor 1/1, model 2/3 (extracts)
i Fold3: preprocessor 1/1, model 2/3 (predictions)
i Fold3: preprocessor 1/1, model 3/3
✓ Fold3: preprocessor 1/1, model 3/3
i Fold3: preprocessor 1/1, model 3/3 (extracts)
i Fold3: preprocessor 1/1, model 3/3 (predictions)
i Fold4: preprocessor 1/1
✓ Fold4: preprocessor 1/1
i Fold4: preprocessor 1/1, model 1/3
✓ Fold4: preprocessor 1/1, model 1/3
i Fold4: preprocessor 1/1, model 1/3 (extracts)
i Fold4: preprocessor 1/1, model 1/3 (predictions)
i Fold4: preprocessor 1/1, model 2/3
✓ Fold4: preprocessor 1/1, model 2/3
i Fold4: preprocessor 1/1, model 2/3 (extracts)
i Fold4: preprocessor 1/1, model 2/3 (predictions)
i Fold4: preprocessor 1/1, model 3/3
✓ Fold4: preprocessor 1/1, model 3/3
i Fold4: preprocessor 1/1, model 3/3 (extracts)
i Fold4: preprocessor 1/1, model 3/3 (predictions)
i Fold5: preprocessor 1/1
✓ Fold5: preprocessor 1/1
i Fold5: preprocessor 1/1, model 1/3
✓ Fold5: preprocessor 1/1, model 1/3
i Fold5: preprocessor 1/1, model 1/3 (extracts)
i Fold5: preprocessor 1/1, model 1/3 (predictions)
i Fold5: preprocessor 1/1, model 2/3
✓ Fold5: preprocessor 1/1, model 2/3
i Fold5: preprocessor 1/1, model 2/3 (extracts)
i Fold5: preprocessor 1/1, model 2/3 (predictions)
i Fold5: preprocessor 1/1, model 3/3
✓ Fold5: preprocessor 1/1, model 3/3
i Fold5: preprocessor 1/1, model 3/3 (extracts)
i Fold5: preprocessor 1/1, model 3/3 (predictions)
> 
> # View the results
> CV_results
# Tuning results
# 5-fold cross-validation 
# A tibble: 5 × 4
  splits               id    .metrics         .notes          
  <list>               <chr> <list>           <list>          
1 <split [26215/6554]> Fold1 <tibble [9 × 6]> <tibble [0 × 3]>
2 <split [26215/6554]> Fold2 <tibble [9 × 6]> <tibble [0 × 3]>
3 <split [26215/6554]> Fold3 <tibble [9 × 6]> <tibble [0 × 3]>
4 <split [26215/6554]> Fold4 <tibble [9 × 6]> <tibble [0 × 3]>
5 <split [26216/6553]> Fold5 <tibble [9 × 6]> <tibble [0 × 3]>
> 
> ## Find Best Tuning Parameters
> bestTune <- CV_results %>%
+ select_best("roc_auc")
> bestTune
# A tibble: 1 × 3
       penalty mixture .config             
         <dbl>   <dbl> <chr>               
1 0.0000000001       0 Preprocessor1_Model1
> 
> ## Finalize the Workflow & fit it
> final_wf <-
+   amazon_workflow %>%
+   finalize_workflow(bestTune) %>%
+   fit(data=amazonTrain)
> 
> ## Predict
> final_wf %>%
+   predict(new_data = amazonTrain, type="class")
# A tibble: 32,769 × 1
   .pred_class
   <fct>      
 1 1          
 2 1          
 3 1          
 4 1          
 5 1          
 6 1          
 7 1          
 8 1          
 9 1          
10 1          
# ℹ 32,759 more rows
> 
> preg_predictions <- predict(final_wf,
+                             new_data=amazonTest,
+                             type="class") # "class" or "prob" (see doc)
> 
> preg_predictions <- cbind(amazonTest$id, preg_predictions) %>%
+   rename(Id = "amazonTest$id",
+          Action = ".pred_class") #%>%
> #select (-"amazonTest$id")
> 
> vroom_write(x=preg_predictions, file="preg2.csv", delim=",")
> 
> proc.time()
    user   system  elapsed 
4628.576   73.396 1640.231 
58   64.822 1677.243 
